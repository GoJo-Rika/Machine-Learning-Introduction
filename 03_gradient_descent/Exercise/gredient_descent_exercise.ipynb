{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a36a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c75742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd39f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_sklearn():\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(df[['math']], df['cs']) #X should always be a 2D array, we cannot pass it as a series - df['math']\n",
    "    return reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ecac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df['math'])\n",
    "y = np.array(df['cs'])\n",
    "def gradient_descent(x,y):\n",
    "    # we can apply the gradient decent algorithm on the pandas series\n",
    "    # but once the values gets bigger, it throws an error as \"Result too large\"\n",
    "    # Because of this, numpy array are better in this kind of scenarios. \n",
    "    # that's the reason why numpy array are used.\n",
    "    m_curr = b_curr = 0\n",
    "    iterations = 1000000\n",
    "    n = len(x)\n",
    "    learning_rate = 0.0002\n",
    "    \n",
    "    cost_previous = 0\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n)*sum((y - y_predicted)**2)\n",
    "        md = -(2/n)*sum(x*(y - y_predicted))\n",
    "        bd = -(2/n)*sum(y - y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        if math.isclose(cost, cost_previous, rel_tol=1e-20):\n",
    "            break\n",
    "        cost_previous = cost\n",
    "#         print(f'm {m_curr}, b {b_curr}, cost {cost}, iterations {i}')\n",
    "    \n",
    "    return m_curr, b_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d560240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gradient Descent Function: Coef 1.0177381667350405 and Intercept 1.9150826165722297\n",
      "Using sklearn: Coef [1.01773624] and Intercept 1.9152193111569176\n"
     ]
    }
   ],
   "source": [
    "m, b = gradient_descent(x,y)\n",
    "print(f'Using Gradient Descent Function: Coef {m} and Intercept {b}')\n",
    "\n",
    "m_sklearn, b_sklearn = predict_using_sklearn()\n",
    "print(f'Using sklearn: Coef {m_sklearn} and Intercept {b_sklearn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec4987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
